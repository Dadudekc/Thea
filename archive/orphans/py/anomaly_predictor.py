import re\nimport json\nimport os\nfrom datetime import datetime, timezone, timedelta\nfrom collections import defaultdict, deque\nfrom typing import Dict, List, Optional, Any\n\n# --- Configuration ---\nLOG_FILES = {\n    \"bridge_system\": \"../logs/bridge_system.log\",\n    \"cursor_output\": \"../logs/cursor_output.log\",\n    \"scraper\": \"../logs/scraper_log.jsonl\"\n}\nOUTPUT_PREDICTIONS_FILE = \"predicted_anomalies.jsonl\"\nOUTPUT_ACTIVITY_LOG = \"../logs/anomaly_predictor_activity.log\"\nALERT_FLAG_FILE = \"../alerts/anomaly_forecast.flag\"\n\nPREDICTION_WINDOW_CYCLES = 5\nHIGH_CONFIDENCE_THRESHOLD = 0.85\nRECENT_ENTRIES_WINDOW = 5\nHIGH_LATENCY_THRESHOLD_MS = 5000 # e.g., 5 seconds\nSTALENESS_THRESHOLD_ENTRIES = 3 # No success in last N entries raises flag\n\n# Error Patterns\nBRIDGE_ERROR_PATTERN = re.compile(r\"^(\\S+) \\[ERROR\\] Module (\\d+).*$\")\nBRIDGE_WARN_PATTERN = re.compile(r\"^(\\S+) \\[WARN\\] Module (\\d+).*$\")\nBRIDGE_INFO_PATTERN = re.compile(r\"^(\\S+) \\[INFO\\] Module (\\d+).*$\") # To track activity\nCURSOR_FAILURE_PATTERN = re.compile(r\"^(\\[\\S+\\]) (?:Injection failed|Execution error|Traceback).*$\", re.IGNORECASE)\nCURSOR_SUCCESS_PATTERN = re.compile(r\"^(\\[\\S+\\]) Execution successful.*$\", re.IGNORECASE)\n\n# Module-Agent Mapping (Best guess based on previous context)\nMODULE_AGENT_MAP = {\n    \"1\": \"RUSTBYTE/IRONVALE/KNURLSHADE\",\n    \"2\": \"HEXMIRE/SPITWIRE\",\n    \"3\": \"KNURLSHADE\",\n    \"4\": \"VEINDRILL\",\n    \"5\": \"RUSTBYTE\",\n    \"6\": \"KNURLSHADE\",\n    \"7\": \"IRONVALE\",\n    \"8\": \"VEINDRILL\",\n    \"Cursor\": \"BridgeInterface(M1/M2)\", # Map cursor logs to relevant interface modules\n    \"Scraper\": \"BridgeInterface(M1/M2)\" # Map scraper logs to relevant interface modules\n}\n\ndef parse_iso_timestamp(ts_string: str) -> Optional[datetime]:\n    \"\"\"Parses ISO timestamp, handling potential Z suffix.\"\"\"\n    try:\n        if ts_string.endswith(\'Z\'):\n            ts_string = ts_string[:-1] + \'+00:00\'\n        # Handle potential brackets from cursor log\n        ts_string = ts_string.strip(\'[]\')\n        return datetime.fromisoformat(ts_string).astimezone(timezone.utc)\n    except (ValueError, TypeError):\n        return None\n\nclass AnomalyPredictor:\n    def __init__(self):\n        # Store recent entries per module {module_id: deque([(timestamp, log_level, details_dict)])}\n        self.module_history = defaultdict(lambda: deque(maxlen=RECENT_ENTRIES_WINDOW))\n        self.predictions = []\n        self.activity_log = []\n\n    def log_activity(self, message: str):\n        ts = datetime.now(timezone.utc).isoformat()\n        log_entry = f\"{ts} [Predictor] {message}\"\n        self.activity_log.append(log_entry)\n        print(log_entry) # Also print to console\n\n    def parse_logs(self):\n        self.log_activity(\"Starting log parsing phase.\")\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n\n        # 1. Parse bridge_system.log\n        try:\n            file_path = os.path.join(script_dir, LOG_FILES[\"bridge_system\"])\n            self.log_activity(f\"Parsing {file_path}\")\n            with open(file_path, \'r\') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line or line.startswith(\'#\'):\n                        continue\n\n                    error_match = BRIDGE_ERROR_PATTERN.match(line)\n                    warn_match = BRIDGE_WARN_PATTERN.match(line)\n                    info_match = BRIDGE_INFO_PATTERN.match(line)\n                    level = None\n                    module_id = None\n                    timestamp = None\n                    details = {\"raw_line\": line}\n\n                    if error_match:\n                        level = \"ERROR\"\n                        timestamp_str, module_id = error_match.groups()\n                    elif warn_match:\n                        level = \"WARN\"\n                        timestamp_str, module_id = warn_match.groups()\n                    elif info_match:\n                        level = \"INFO\"\n                        timestamp_str, module_id = info_match.groups()\n\n                    if level and module_id:\n                        timestamp = parse_iso_timestamp(timestamp_str)\n                        if timestamp:\n                            self.module_history[module_id].append((timestamp, level, details))\n        except FileNotFoundError:\n             self.log_activity(f\"Warning: File not found {file_path}\")\n        except Exception as e:\n            self.log_activity(f\"Error parsing {LOG_FILES[\'bridge_system\']}: {e}\")\n\n        # 2. Parse cursor_output.log\n        try:\n            file_path = os.path.join(script_dir, LOG_FILES[\"cursor_output\"])\n            self.log_activity(f\"Parsing {file_path}\")\n            with open(file_path, \'r\') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line or line.startswith(\'#\'):\n                        continue\n\n                    fail_match = CURSOR_FAILURE_PATTERN.match(line)\n                    success_match = CURSOR_SUCCESS_PATTERN.match(line)\n                    level = None\n                    timestamp = None\n                    details = {\"raw_line\": line}\n\n                    if fail_match:\n                        level = \"ERROR\"\n                        timestamp_str = fail_match.group(1)\n                    elif success_match:\n                        level = \"INFO\" # Treat success as INFO\n                        timestamp_str = success_match.group(1)\n\n                    if level:\n                        timestamp = parse_iso_timestamp(timestamp_str)\n                        if timestamp:\n                            # Attribute to a generic 'Cursor' module impacting M1/M2\n                            self.module_history[\"Cursor\"].append((timestamp, level, details))\n        except FileNotFoundError:\n             self.log_activity(f\"Warning: File not found {file_path}\")\n        except Exception as e:\n            self.log_activity(f\"Error parsing {LOG_FILES[\'cursor_output\']}: {e}\")\n\n        # 3. Parse scraper_log.jsonl\n        try:\n            file_path = os.path.join(script_dir, LOG_FILES[\"scraper\"])\n            self.log_activity(f\"Parsing {file_path}\")\n            with open(file_path, \'r\') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        data = json.loads(line)\n                        timestamp_str = data.get(\"timestamp_utc\")\n                        status = data.get(\"status\")\n                        latency = data.get(\"latency_ms\")\n                        timestamp = parse_iso_timestamp(timestamp_str)\n                        level = \"ERROR\" if status == \"error\" else \"INFO\"

                        if timestamp:\n                            # Attribute to 'Scraper' module impacting M1/M2
                            self.module_history[\"Scraper\"].append((timestamp, level, details))\n                    except json.JSONDecodeError:\n                        self.log_activity(f\"Warning: Invalid JSON in {LOG_FILES[\'scraper\']}: {line}\")\n                    except Exception as e_inner:\
                         self.log_activity(f\"Error processing line in {LOG_FILES[\'scraper\']}: {line} - {e_inner}\")\n        except FileNotFoundError:\n             self.log_activity(f\"Warning: File not found {file_path}\")\n        except Exception as e:\n            self.log_activity(f\"Error parsing {LOG_FILES[\'scraper\']}: {e}\")\n\n        self.log_activity(\"Log parsing phase complete.\")\n\n    def predict_anomalies(self):\n        self.log_activity(\"Starting anomaly prediction phase.\")\n        self.predictions = []\n        now = datetime.now(timezone.utc) # Reference for staleness - less critical now\n\n        for module_id, history in self.module_history.items():\n            if not history:\n                continue\n\n            recent_errors = 0\n            recent_warnings = 0\n            recent_high_latency = 0\n            entries_in_window = len(history)\n            has_recent_success = False\n            most_recent_level = history[-1][1]\n\n            for i, (ts, level, details) in enumerate(reversed(history)):\n                if level == \"ERROR\":\n                    recent_errors += 1\n                if level == \"WARN\":\n                    recent_warnings += 1 # Consider warnings as contributing factor\n                if level == \"INFO\":\n                     has_recent_success = True\n\n                # Check latency for Scraper module\n                if module_id == \"Scraper\" and isinstance(details.get(\"latency_ms\"), (int, float)):\n                    if details[\"latency_ms\"] > HIGH_LATENCY_THRESHOLD_MS:\n                        recent_high_latency += 1\n                \n                # Check staleness based on lack of success in window\n                if i + 1 >= STALENESS_THRESHOLD_ENTRIES and not has_recent_success:\n                    staleness_flag = True\n                else:\n                    staleness_flag = False\n\n            # --- Calculate Confidence Score --- \n            confidence = 0.0\n            rationale = []\n\n            # 1. Error Score (Weight: 0.6)\n            error_ratio = recent_errors / entries_in_window\n            confidence += 0.6 * error_ratio\n            if error_ratio > 0: rationale.append(f\"{recent_errors}/{entries_in_window} recent entries were ERRORs.\")\n\n            # 2. Warning Score (Weight: 0.1) - Lower weight for warnings\n            warning_ratio = recent_warnings / entries_in_window\n            confidence += 0.1 * warning_ratio\n            if warning_ratio > 0: rationale.append(f\"{recent_warnings}/{entries_in_window} recent entries were WARNs.\")\n\n            # 3. Latency Score (Weight: 0.3) - Only for Scraper/Interface\n            if module_id in [\"Scraper\", \"Cursor\"]: # Apply latency check where relevant\n                 latency_ratio = recent_high_latency / entries_in_window\n                 confidence += 0.3 * latency_ratio\n                 if latency_ratio > 0: rationale.append(f\"{recent_high_latency}/{entries_in_window} recent events had high latency.\")\n            \n            # 4. Staleness Boost (Adds 0.2 confidence if stale)\n            if staleness_flag:\n                confidence += 0.2 \n                rationale.append(f\"No SUCCESS found in last {STALENESS_THRESHOLD_ENTRIES} entries.\")\n\n            # 5. Most Recent Event Boost (Big boost if last event was ERROR)\n            if most_recent_level == \"ERROR\":\n                confidence += 0.5\n                rationale.append(\"Most recent event was ERROR.\")\n            elif most_recent_level == \"WARN\":\n                 confidence += 0.1 # Smaller boost for warning\n                 rationale.append(\"Most recent event was WARN.\")\n\n            # Cap confidence at 1.0\n            confidence = min(confidence, 1.0)\n            \n            # Only record prediction if confidence > 0 (or some low threshold)\n            if confidence > 0.1: \n                agent = MODULE_AGENT_MAP.get(module_id, \"Unknown\")\n                prediction = {\n                    \"module_id\": module_id,\n                    \"agent\": agent,\n                    \"confidence_score\": round(confidence, 3),\n                    \"rationale\": \" | \".join(rationale) if rationale else \"General trend analysis.\",\n                    \"prediction_window\": PREDICTION_WINDOW_CYCLES\n                }\n                self.predictions.append(prediction)\n                self.log_activity(f\"Prediction generated for Module {module_id}: Score={confidence:.3f}\")\n\n        self.log_activity(\"Anomaly prediction phase complete.\")\n\n    def write_output(self):\n        self.log_activity(\"Writing prediction and activity logs.\")\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        \n        # Write predictions\n        output_path = os.path.join(script_dir, OUTPUT_PREDICTIONS_FILE)\n        try:\n            with open(output_path, \'w\') as f:\n                for prediction in self.predictions:\n                    json.dump(prediction, f)\n                    f.write(\'\\n\')\n            self.log_activity(f\"Predictions written to {output_path}\")\n        except IOError as e:\n            self.log_activity(f\"Error writing predictions file {output_path}: {e}\")\n\n        # Check for high confidence and create flag file\n        high_confidence_found = any(p[\"confidence_score\"] > HIGH_CONFIDENCE_THRESHOLD for p in self.predictions)\n        if high_confidence_found:\n             flag_path = os.path.join(script_dir, ALERT_FLAG_FILE)\n             alert_dir = os.path.dirname(flag_path)\n             if not os.path.exists(alert_dir):\n                 os.makedirs(alert_dir)\n             try:\n                 with open(flag_path, \'w\') as f:\n                    f.write(f\"Anomaly forecast triggered at {datetime.now(timezone.utc).isoformat()}\\n\")\n                    f.write(f\"High confidence prediction found. See {OUTPUT_PREDICTIONS_FILE}\\n\")\n                 self.log_activity(f\"High confidence prediction detected. Alert flag created at {flag_path}\")\n             except IOError as e:\n                 self.log_activity(f\"Error writing alert flag file {flag_path}: {e}\")\n        \n        # Write activity log\n        activity_path = os.path.join(script_dir, OUTPUT_ACTIVITY_LOG)\n        log_dir = os.path.dirname(activity_path)\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        try:\n            with open(activity_path, \'a\') as f:\n                for entry in self.activity_log:\n                    f.write(entry + \'\\n\')\n            # Don't log this success to the file itself to avoid infinite loop potential\n            print(f\"Activity logged to {activity_path}\") \n        except IOError as e:\n             print(f\"Error writing activity log file {activity_path}: {e}\") # Print error only\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    predictor = AnomalyPredictor()\n    predictor.log_activity(\"--- Anomaly Predictor Run Starting ---\")\n    predictor.parse_logs()\n    predictor.predict_anomalies()\n    predictor.write_output()\n    predictor.log_activity(\"--- Anomaly Predictor Run Finished ---\")\n    # Write final activity log state again after finish message\n    try:\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        activity_path = os.path.join(script_dir, OUTPUT_ACTIVITY_LOG)\n        with open(activity_path, \'a\') as f:\n            f.write(predictor.activity_log[-1] + \'\\n\') # Write the finish message\n    except Exception:\n        pass # Avoid errors here if logging failed earlier
