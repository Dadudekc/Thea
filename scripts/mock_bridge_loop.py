#!/usr/bin/env python3
"""
mock_bridge_loop.py

A mock implementation of the dreamos.bridge.run_bridge_loop.py script.
This script is intended for testing agent logic that relies on the self-prompting
procedure without making live calls to external LLMs or using Selenium.

It reads a prompt from a specified file, generates a predefined or simple
mocked response, and writes this response to the standard output location
expected by agents (runtime/bridge_outbox/).
"""

import argparse
import json
import sys  # Added import
import time
from datetime import datetime
from pathlib import Path

# Default output directory as observed from run_e2e_bridge_test.py
DEFAULT_OUTBOX_DIR = Path("runtime/bridge_outbox")


def generate_mock_response(
    prompt_text: str, agent_id: str
) -> str:  # Return type is str
    """
    Generates a mock response string in hybrid format (text + JSON block).
    """
    text_part = f'This is a mocked response from Agent {agent_id} to your prompt: "{prompt_text[:100]}..."'
    memory_update = {
        "mock_status": "success",
        "notes": "This response was generated by mock_bridge_loop.py.",
        "original_prompt_snippet": prompt_text[:50],
    }

    hybrid_response_str = (
        f"{text_part}\n\n```json\n{json.dumps(memory_update, indent=2)}\n```"
    )
    return hybrid_response_str


def main():
    parser = argparse.ArgumentParser(description="Mock Dream.OS Bridge Loop")
    parser.add_argument("--agent-id", required=True, help="The ID of the agent.")
    parser.add_argument(
        "--prompt-file",
        required=True,
        type=Path,
        help="Path to the file containing the prompt.",
    )
    parser.add_argument(
        "--response-timeout",
        type=int,
        default=90,
        help="Timeout in seconds (mostly ignored by mock).",
    )

    args = parser.parse_args()

    print(f"[MockBridgeLoop] Agent ID: {args.agent_id}")
    print(f"[MockBridgeLoop] Prompt File: {args.prompt_file}")
    print(f"[MockBridgeLoop] Timeout: {args.response_timeout}s (ignored)")

    if not args.prompt_file.exists():
        print(f"[MockBridgeLoop] ERROR: Prompt file not found: {args.prompt_file}")
        return 1

    try:
        prompt_text = args.prompt_file.read_text(encoding="utf-8").strip()
        if not prompt_text:
            print("[MockBridgeLoop] WARNING: Prompt file is empty.")
    except Exception as e:
        print(f"[MockBridgeLoop] ERROR: Could not read prompt file: {e}")
        return 1

    print(f'[MockBridgeLoop] Read prompt: "{prompt_text[:150]}..."')

    mock_response_content_str = generate_mock_response(prompt_text, args.agent_id)

    timestamp_str = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    output_data = {
        "agent_id": args.agent_id,
        "prompt": prompt_text,
        "response": mock_response_content_str,
        "timestamp": timestamp_str,
        "mocked_by": "mock_bridge_loop.py",
    }

    DEFAULT_OUTBOX_DIR.mkdir(parents=True, exist_ok=True)

    agent_id_str = str(args.agent_id)  # Ensure it's a string
    if agent_id_str.startswith("Agent-"):
        output_filename = f"{agent_id_str}_{timestamp_str}.json"
    else:
        output_filename = f"agent{agent_id_str}_{timestamp_str}.json"
    output_file_path = DEFAULT_OUTBOX_DIR / output_filename

    try:
        with open(output_file_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2)
        print(
            f"[MockBridgeLoop] Mock response successfully written to: {output_file_path}"
        )
    except Exception as e:
        print(f"[MockBridgeLoop] ERROR: Could not write mock response file: {e}")
        return 1

    time.sleep(0.1)
    print("[MockBridgeLoop] Processing complete.")
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
