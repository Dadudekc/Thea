# Agent Development Protocol: Iterative Refinement Workflow

This document outlines the standard iterative workflow used for developing and debugging as an agent within the Dream.OS Team, particularly when involving planning and tool execution.

This workflow emphasizes proactivity, continuous testing, and incremental enhancement.

## Core Loop: Prompt → Plan → Execute → Validate → Commit (PPEC-V)

The fundamental cycle for adding or refining agent functionality:

1.  **Prompt:** Define the desired capability or fix using a clear, high-level task description (natural language).
    *   *Example:* `"Refactor class MemoryManager in core/memory/memory_manager.py"`

2.  **Plan:** Utilize the appropriate planning mechanism (e.g., `ContextPlannerTool`) to translate the prompt into a sequence of concrete tool execution steps.
    *   *Initial Goal:* Generate the *expected* plan (e.g., `read_file`, `write_file`).
    *   *Debugging:* If the plan is incorrect (wrong tools, wrong arguments, wrong order, fallback triggered unexpectedly), analyze the planner's logic:
        *   Check keyword matching (`if 'refactor' in task...`).
        *   Verify entity/target extraction (`_extract_targets`).
        *   Examine rule conditions and order.
        *   Refine the planner rules, commit the fix, and **re-run the planning step** with the original prompt until the plan is correct.

3.  **Execute:** Pass the generated plan to the execution mechanism (e.g., `ToolExecutionAgent` using the real `ToolRegistry`).
    *   *Initial Goal:* Execute the plan using real tools against the actual codebase/filesystem.
    *   *Debugging:* If execution fails:
        *   **Identify the failing step and tool** from logs/output.
        *   **Analyze the error:**
            *   *Tool Error (e.g., `FileNotFoundError`, `UnicodeEncodeError`, `APIError`):* Debug the specific tool's implementation (e.g., path handling in `FileReadTool`, encoding in logging, API call logic in a service tool). Fix the tool, commit, and **re-run the execution step** with the *previously validated* plan.
            *   *Executor Error:* Debug the `ToolExecutionAgent` itself (e.g., how it passes context, handles results, loops through steps). Fix the executor, commit, and re-run.
            *   *Unexpected Tool Behavior:* If the tool *succeeded* but did the wrong thing (e.g., `write_file` wrote incorrect content), review the *arguments* generated by the **Planner** in Step 2. This often means the Plan step needs refinement, not the execution/tool step.

4.  **Validate:** Check the results of the execution against the initial Prompt and ensure application integrity.
    *   **Functional Check:** Did the intended change occur? (e.g., Was the file created/modified correctly? Was the data retrieved?)
    *   **Side Effects Check:** Are there any unintended side effects?
    *   **Verification Methods:** Use `git status`, `git diff`, file inspection.
    *   **Runtime Check:** **Crucially, run the relevant application entry point (`python main.py` for GUI, `python main.py --run-task ...` for task execution, specific test scripts, etc.) to catch runtime errors (like `ImportError`, `NameError`, `TypeError`) introduced by the changes.** This is especially important after modifying UI code or core component interactions.
    *   *Debugging:* If validation fails:
        *   If the functional change is incorrect, determine if the root cause was an incorrect **Plan** (Step 2) or flawed **Execution/Tool** logic (Step 3) and return to that step.
        *   If a runtime error occurs (e.g., GUI doesn't launch), analyze the traceback to pinpoint the faulty code (often in the modified files or their imports). Debug and fix the runtime error, commit the fix, and **repeat the Runtime Check**.

5.  **Commit:** Once the entire cycle (Prompt → Plan → Execute → Validate, including Runtime Check) succeeds for the specific task, commit the relevant changes (planner rules, tool fixes, executor logic, new components, runtime fixes) with clear, atomic commit messages.

## Proactivity Principle

Throughout this loop, the assisting agent (like Gemini) should strive for proactivity:

*   **Anticipate Next Steps:** If a fix is made (e.g., fixing planner logic), anticipate that the user will want to immediately re-test the planning/execution. Initiate the test.
*   **Identify Missing Components:** If a plan relies on a non-existent file or tool, proactively identify this (e.g., using `list_dir`, checking the registry) and offer to create/implement it based on the inferred intent.
*   **Utilize Available Tools:** Prefer using code-reading, searching, or file-listing tools to gather context before asking the user for information that might be present in the codebase.
*   **Self-Correction:** If a step fails due to an error the assistant introduced (e.g., syntax error, incorrect logic), take initiative to fix it before requiring user intervention.

## Iteration and Scaling

This PPEC-V loop forms the basis for incrementally building more complex agent behaviors. Each successful cycle adds a new capability or refines an existing one, making the agent progressively more intelligent and reliable.

*   Start with simple, core tasks (read, write, create).
*   Gradually add rules for more complex operations (refactor, migrate, extract, analyze).
*   Continuously test the live execution pipeline after each significant change. 